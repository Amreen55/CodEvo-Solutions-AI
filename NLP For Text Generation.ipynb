{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de146b15-c2e5-47bd-aecd-c3bd93f24882",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "209e90a0-362d-4996-a950-e9ab227771d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
      "\u001b[1m1115394/1115394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1us/step\n"
     ]
    }
   ],
   "source": [
    "path_to_file = tf.keras.utils.get_file(\n",
    "    'shakespeare.txt',\n",
    "    'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt',\n",
    ")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46fc74f0-fbf4-480b-8740-9124f37b152d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "print(f\"Lenght of text: {len(text)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "701b90e8-c0da-4d51-ad8a-dcf01333d97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33f8427c-7abe-4639-953a-8a56ca75abad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 unique characters\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} unique characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e41431f4-f12a-4c92-8529-4fd7fbd4733e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b's', b't', b'e', b'f', b'a', b'n'],\n",
       " [b'n', b'a', b'f', b'e', b't', b's']]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_text = ['stefan', 'nafets']\n",
    "\n",
    "chars = tf.strings.unicode_split(example_text, input_encoding='UTF-8')\n",
    "chars\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a09e8e5d-d1b7-4e00-b282-39159b20ae4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_from_chars = tf.keras.layers.StringLookup(\n",
    "    vocabulary=list(vocab),\n",
    "    mask_token=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81bdc2b0-4eed-4b43-8660-c392ca781b18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[58, 59, 44, 45, 40, 53],\n",
       " [53, 40, 45, 44, 59, 58]]>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = ids_from_chars(chars)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cea2d72-ff6e-40fd-beb8-ae861d94266f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_from_ids = tf.keras.layers.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(),\n",
    "    invert=True,\n",
    "    mask_token=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d243fe0b-3108-4a83-995c-fcc6828aca9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b's', b't', b'e', b'f', b'a', b'n'],\n",
       " [b'n', b'a', b'f', b'e', b't', b's']]>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = chars_from_ids(ids)\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "393411de-7fe9-4034-bc02-f092084ea613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'stefan', b'nafets'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.reduce_join(chars, axis=1).numpy()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4b43cd0-5c09-44f2-99af-2210a3b1c65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "  return tf.strings.reduce_join(chars_from_ids(ids), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d25c73b-67f0-404d-99b3-5ac8addad1d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1], dtype=int64)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d34f50b-dc6e-460c-8380-0e475c464c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24e5d932-64bb-4bd7-aca2-8e896995a270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n",
      " \n",
      "C\n",
      "i\n",
      "t\n",
      "i\n"
     ]
    }
   ],
   "source": [
    "for ids in ids_dataset.take(10):\n",
    "  print(chars_from_ids(ids).numpy().decode('UTF-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d6c366a-b10c-4020-8892-dee68f1507ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "examples_per_epoch = len(text) // (seq_length + 1)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44a0f42a-35cd-4b1d-8060-b14cbc13019a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
      " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
      " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
      " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
      " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
      " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
      " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
      " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "sequences = ids_dataset.batch(seq_length + 1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(1):\n",
    "  print(chars_from_ids(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f984436-935c-40b5-8dd6-5f86b109d690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
      "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
      "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences.take(5):\n",
    "  print(text_from_ids(seq).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4051231a-93d8-4c06-a7e7-8cd74fa54122",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "  input_text = sequence[:-1]\n",
    "  target_text = sequence[1:]\n",
    "  return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a8af1f0-ebca-42f0-ad10-2c607baac962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
       " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_input_target(list('Tensorflow'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "527217ce-12bc-4550-b33c-2d5da25be03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e335924f-63bb-41c0-9e90-fbdf02124dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input  : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "Target : b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "  print('Input  :', text_from_ids(input_example).numpy())\n",
    "  print('Target :', text_from_ids(target_example).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ced2eea-4c78-4002-82bf-8a70c2c02f2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=(TensorSpec(shape=(63, 100), dtype=tf.int64, name=None), TensorSpec(shape=(63, 100), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 63\n",
    "\n",
    "\"\"\"Buffer size to shuffle the dataset\n",
    "(TF data is designed to work wiht possobly infinite sequences,\n",
    "so it dosen't attempt to shuffle the entire sequence in memory. Instadfe\n",
    "it maintaines a buffer is which it shuffles elements)\"\"\"\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset.shuffle(BUFFER_SIZE) \\\n",
    "    .batch(BATCH_SIZE, drop_remainder=True) \\\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4746996e-4b78-4d0f-b58f-d1d681ba2359",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(\n",
    "            rnn_units,\n",
    "            return_sequences=True,\n",
    "            return_state=True\n",
    "        )\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, inputs, states=None, return_state=False, training=False):\n",
    "        x = self.embedding(inputs)\n",
    "        \n",
    "        # Pass inputs through GRU layer\n",
    "        x, states = self.gru(x, initial_state=states, training=training)\n",
    "        \n",
    "        # Pass GRU outputs through Dense layer\n",
    "        x = self.dense(x)\n",
    "\n",
    "        if return_state:\n",
    "            return x, states\n",
    "        else:\n",
    "            return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9e7fb189-9c62-41cc-8ff0-0800fc6ba7fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"my_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"my_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                            │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ (\u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)              │         \u001b[38;5;34m256,000\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ gru (\u001b[38;5;33mGRU\u001b[0m)                            │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> (1000.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m256,000\u001b[0m (1000.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> (1000.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m256,000\u001b[0m (1000.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5a64cc5f-12a0-4e99-8820-b695bc02986a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63, 100, 10000) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "        super().__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(\n",
    "            rnn_units,\n",
    "            return_sequences=True,\n",
    "            return_state=True\n",
    "        )\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "    \n",
    "    def call(self, inputs, states=None, return_state=False, training=False):\n",
    "        x = self.embedding(inputs)\n",
    "\n",
    "        # Initialize states if not provided\n",
    "        if states is None:\n",
    "            batch_size = tf.shape(inputs)[0]  # Get the batch size from the inputs\n",
    "            states = tf.zeros((batch_size, self.gru.units))  # Initialize the states manually with the correct shape\n",
    "\n",
    "        # Run GRU layer\n",
    "        x, states = self.gru(x, initial_state=states, training=training)\n",
    "\n",
    "        # Output dense layer\n",
    "        x = self.dense(x)\n",
    "\n",
    "        # Return both outputs and states if required\n",
    "        if return_state:\n",
    "            return x, states\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "# Assuming the vocabulary size, embedding dimension, and RNN units are set\n",
    "vocab_size = 10000\n",
    "embedding_dim = 256\n",
    "rnn_units = 1024\n",
    "\n",
    "# Create the model\n",
    "model = MyModel(vocab_size, embedding_dim, rnn_units)\n",
    "\n",
    "# Test the model on an input batch\n",
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(\n",
    "        example_batch_predictions.shape,\n",
    "        \"# (batch_size, sequence_length, vocab_size)\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eacca98e-bcd8-4cb6-ad13-bc7f097eae16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63, 100, 10000) # (batch_size, sequence_length, vocab_size)\n",
      "Input:\n",
      "b', his unfeigned friend,\\nThat, if King Lewis vouchsafe to furnish us\\nWith some few bands of chosen so'\n",
      "Next Char Predictions:\n",
      "b'[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK]s[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK]'\n"
     ]
    }
   ],
   "source": [
    "# Assuming you already have a batch of predictions\n",
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    # Get predictions from the model for the input batch\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    \n",
    "    # Print the shape of the output predictions\n",
    "    print(\n",
    "        example_batch_predictions.shape,\n",
    "        \"# (batch_size, sequence_length, vocab_size)\"\n",
    "    )\n",
    "    \n",
    "    # Sampling the next character predictions from the logits\n",
    "    sampled_indices = tf.random.categorical(\n",
    "        example_batch_predictions[0],  # Use the first sample in the batch\n",
    "        num_samples=1\n",
    "    )\n",
    "    \n",
    "    # Squeeze to remove extra dimensions\n",
    "    sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
    "    \n",
    "    # Print input and sampled next characters\n",
    "    print('Input:', text_from_ids(input_example_batch[0]).numpy(), sep='\\n')\n",
    "    print('Next Char Predictions:', text_from_ids(sampled_indices).numpy(), sep='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "16257bf0-4ce3-4909-aa08-dafd4ee72261",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c4a93800-4e00-4f39-9563-867d1c902352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediciton shape: \n",
      "(63, 100, 10000)\n",
      "# (batch_size, sequence_length, vocab_size\n",
      "New loss:         tf.Tensor(9.210249, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\n",
    "    'Prediciton shape: ',\n",
    "    example_batch_predictions.shape,\n",
    "    '# (batch_size, sequence_length, vocab_size',\n",
    "    sep='\\n'\n",
    ")\n",
    "print('New loss:        ', example_batch_mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4ad33439-108e-43bd-ae22-989d2b6b4d9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9999.086"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.exp(example_batch_mean_loss).numpy()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aeba95c6-1278-4551-acc6-576638c13d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "212b8488-5be1-4d5e-9cd4-b378e4c9a322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the checkpoint file with the correct extension\n",
    "checkpoint_prefix = os.path.join(checkpoint_path, \"ckpt_epoch_{epoch:02d}.weights.h5\")\n",
    "\n",
    "# Create the ModelCheckpoint callback\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True  # Corrected argument\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ce7b7cd4-8e6e-46c9-bb0a-72692aa26b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2600s\u001b[0m 15s/step - loss: 4.0092\n",
      "Epoch 2/10\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3051s\u001b[0m 17s/step - loss: 2.6908\n",
      "Epoch 3/10\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1486s\u001b[0m 8s/step - loss: 2.4985\n",
      "Epoch 4/10\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2351s\u001b[0m 13s/step - loss: 2.4103\n",
      "Epoch 5/10\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2226s\u001b[0m 13s/step - loss: 2.3539\n",
      "Epoch 6/10\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4362s\u001b[0m 25s/step - loss: 2.3189\n",
      "Epoch 7/10\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3643s\u001b[0m 21s/step - loss: 2.3025\n",
      "Epoch 8/10\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21161s\u001b[0m 121s/step - loss: 2.2774\n",
      "Epoch 9/10\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1499s\u001b[0m 9s/step - loss: 2.2619\n",
      "Epoch 10/10\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1691s\u001b[0m 10s/step - loss: 2.2428\n",
      "CPU times: total: 10h 51min 13s\n",
      "Wall time: 12h 14min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "EPOCHS = 10\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4a968dfa-ce1f-42bf-90b3-a9f1af816ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "    def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.model = model\n",
    "        self.chars_from_ids = chars_from_ids\n",
    "        self.ids_from_chars = ids_from_chars\n",
    "\n",
    "        # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "        skip_ids = self.ids_from_chars([\"[UNK]\"])[:, None]\n",
    "        sparse_mask = tf.SparseTensor(\n",
    "            # Put a -inf at each bad index.\n",
    "            values=[-float(\"inf\")] * len(skip_ids),\n",
    "            indices=skip_ids,\n",
    "            # Match the shape to the vocabulary\n",
    "            dense_shape=[len(ids_from_chars.get_vocabulary())],\n",
    "        )\n",
    "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "    @tf.function\n",
    "    def generate_one_step(self, inputs, states=None):\n",
    "        # Convert strings to token IDs.\n",
    "        input_chars = tf.strings.unicode_split(inputs, \"UTF-8\")\n",
    "        input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "        # Run the model.\n",
    "        # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "        predicted_logits, states = self.model(\n",
    "            inputs=input_ids, states=states, return_state=True\n",
    "        )\n",
    "        # Only use the last prediction.\n",
    "        predicted_logits = predicted_logits[:, -1, :]\n",
    "        predicted_logits = predicted_logits / self.temperature\n",
    "        # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "        predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "        # Sample the output logits to generate token IDs.\n",
    "        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "        # Convert from token ids to characters\n",
    "        predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "        # Return the characters and model state.\n",
    "        return predicted_chars, states\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fed6060e-fbc9-4bf4-acda-0577fd46f1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "68945f37-ddba-42db-99b9-4bb3df1b3ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "Anding Loun croke you, wase't Hars, triengay theed mastanges, maskeN to hif's fon thitherf serme sheuch your\n",
      "that eed a po ofpered that in\n",
      "Whous sailn, fom the be thum, and to herret but sour.\n",
      "\n",
      "MENVINO:\n",
      "On the manning I dirg'ds:\n",
      "Thes sifvit nik o foll the sime tith liff dake:\n",
      "\n",
      "astile I hey, blieg watie vame.\n",
      "\n",
      "COLIO:\n",
      "O, youl dease.\n",
      "\n",
      "MURIUS:\n",
      "Oo speef,\n",
      "Lo lond I ho knomand, sher quke\n",
      "Seepale the may brees grears,\n",
      "Whe lavening werver, anv thes the toous?\n",
      "\n",
      "MORNCIO:\n",
      "Mestlen, 't is then wish:\n",
      "Then thie mroun of lose thes your.\n",
      "\n",
      "DUKES:\n",
      "Se mo eve\n",
      "Ad Voodf the serifnout\n",
      "Do the kinc, thould stawid is mave ig gho dain? bit too,\n",
      "Ipatet with crees, the lass ig\n",
      "Oven, eakn live, me youn beat dere, e brow his denens.\n",
      "\n",
      "CETon his not gomtherint to theid deate tond.\n",
      "The prave arr me treich ou mor, makn's aigh,\n",
      "Thot, there in in Palconce a cojnicoong?\n",
      "\n",
      "HUCHIO:\n",
      "Ho der a woms fnom yrem on uther reesing, thenm-patice gonoropest he alm ad of that,\n",
      "Bat hart, fee wher lathert to dorcebe Nother, Ghange my har,\n",
      "L \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 4.16226863861084\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "    next_char, states = one_step_model.generate_one_step(\n",
    "        next_char, states = states\n",
    "    )\n",
    "    result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), \"\\n\\n\" + \"_\" * 80)\n",
    "print(\"\\nRun time:\", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e6e28714-b10f-45d7-aefc-f28b8752e6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b\"ROMEO:\\nGut Morpery beecr's megeforge, and paining.\\n\\nATROUE:\\nO sord for aver oface, E\\nTove putsst thy Onjuw de tollf 'to ther whent then ence\\nthee yow thome asverd\\nThe mxeme, all theme if bete, ard Sat yiur.\\nBus htarkied, dy tour that ucnctope.\\n\\nGRRINGULES:\\nFor evulles fishendy bent and, ghath hat hese my bolt,?\\nNut me seld apr sent weis nead how\\nyour me noth lile vist tod gate foble Kisn?\\nBuse ofreers, and gads, slain rawn!\\n\\nABENLERI:\\nI sau, wow got my doo, Triund recins to misperous, thou hamak\\nTo haved gon'd a maylf; the devem now\\nIL\\nYom.\\n\\nCRUUCINI EFNAN:\\nWhe the intery gore we porn.\\nHerurl wid loung, frith ta tho kidg dean of\\nThat'ernss bed, to mest un there curorr brownes and the prace offorce.\\nUSThe go,\\nWill mins wom! an the ken wrich whin.\\nHever, you, hag mese with, preacarse theiser, in farourd\\nay youg wisben my preate,\\nAnd shain, pray sill eak\\nyou too in the peut love freachscorcast.\\n\\nARENTES:\\n'dain: it od ut king fod taden, Iwan tree,\\nLeyt most dill notr grure exbat is gutw nothe's\"\n",
      " b\"ROMEO:\\nNow houster, and Glaye,';\\nMimp niry mantsing.\\nMane the ear, tiong! If mroy.\\n\\nENESEN:\\nHels, nit, up,\\nSe meat ut cokn to then of heard's the det!\\nI, 'como frim thater!\\n\\nPortine may withe u't our hits un\\nYour andest oferce if the a beversers sabled everce of offiugsh,\\nYoul shence\\nThe wild fare our not ford, tray, pateare.\\n\\nSARD BINGE:\\nNooted and fid night. My louther mry usfores:\\nIven weing de ze ant to thear coont,\\nWhoult an the abs blestU'd with him. Hert,\\nAnd Jozer.\\n\\nSher, Gok the king\\nSo lend to me see, o me pean ndow caeng kakn in;-\\nKereneI 'd noz past dithrik ave wist, tay merd pruce\\nfor ay\\nle bis lonouband my amcond bager\\nTheny ther ento't flow of this sings, dot for ma dito with of yoo beins; say con.\\n\\nLhever, be kiend, you thou and wieght,\\nBemvling wim now Nord, amy fathe in so deneror them's d dile,\\nTo coly bowtat 'vere him; nigy!\\n\\nCENRONLE:\\nI my condemt\\nSling,\\nG't you uctrie now sangen I har concond, lifsur,\\nThe hid in to sher uin,\\nAnd I hers bake thou hing didle,\\nThat the so\"\n",
      " b\"ROMEO:\\nSull not ol Miscising not cony, heves alo.\\n\\nGirg:\\nThe condiol, Henenget if on Goure ay is in not:\\nAnd sen: my jonst be to ut Marry-y, Gream.\\nYear you: coor of a crouxt not yout,\\nind of him my noth innt bere you, so hou then main digharrkn\\nChres Pacinsone.\\n\\nVher thum so limesr for, richem.\\n\\nBERTOND:\\nis you'd le goth in thount wandy handt.\\n\\nSurss thing\\nI kenowh canco mantengaly.\\n\\nCars my for yok und theme, and peard\\nHizs lorath my have wele liven you wall mens not whead lood'ma,\\nTichice, Ant you maye,\\nHas sear aw mera theive ard it hist all I he reay\\nAnd woweste site offon meres. Ta you here you qurned chaols, at dust!\\n\\nYORCET:\\nYourf botker's vasisu's Ind butty sopner, ong to peakh hisence blord\\nHertren sak:\\nmave CoMe you lord. But hen here! You sell the iting\\nThe nond-ren.\\nAnd thitke it so hertring this deid.\\n\\nCARTHOLDUSLE:\\nShy dew of one but sell porgeatler arfoueftlf of for thy nothty sotsen;\\nYou a disussuns my\\nend thim bendren, anfiellly,\\nYou, woug ligarist, geford.\\n\\nPLoMPHIO:\\nWhat\"\n",
      " b\"ROMEO::\\nGlowe to disht fave and Sencusp,\\nAnd to wo aday I mor, ind anj thougst:\\nFroe sempew, wo notle's in the inchore in the loush tore as myoss:\\nIf yours ma mefce sthes secs an will Wishy uncedser, and tanow's of to cald\\nFer benanow thate ar it your, Irelowen to the in:\\nThee or mres and chat.\\n\\nNINTE:\\nThe safelf ti'd muptrey firusst and loves enestrerel;\\nOnd era re cantion, I, potily shemeare\\ngo san: atitithis to sa stild wersset tour sisw,\\nMo sire of in a gorco; I am off the quuusur's\\nThat deverbis: in no antace lomesmy scome.\\n\\nLIONANE:\\nDot by sin\\nTo make noth by so I mase comion foreres my by a ractor; a'd all your, il you andsest\\nand Padinging, my s me trise av nous\\nfor host is, you ara\\nAnd yous with yuu\\nLerstlincawesed.\\n\\nMINCANLY ENGRUS:\\nLith me e ganged ene\\nThe comans my atr agncanes is.\\n\\nBARTENCYAMUS:\\nThim makn theee ase in o have is fiserr inon's a nos of offfren live's planded comburs he Grold. Havenoth thise ance, by latt of ans of trembly ines\\nTin, haver'ms serce the daid\\nHabm, a\"\n",
      " b\"ROMEO:\\nTher plood fack with the good cides tole's of I gevey led wand sow lead cunou fiors.\\nYit, 'gen sear-ve!\\n\\nFOURI IL:\\nHastror Cupenit so hand, Bontords.\\nNget, that treencat, takerses, gheister, pear now in with him'st.\\n\\nMRRABELTIO:\\nThis noce, I a magy of the oichsrer,\\nAnd, tho dast in geam ming, an\\nWiTh pipund wont\\nLich of Gonis wres southersw thevers fos? Rory, te conemy\\nNos Sane ve sporte, of her you to is fompus will:\\nLringwes fave you nod, not Flats;\\nBy hoth toolk: net wize wis thou?\\n\\nSeNIS:\\nMerervame my weachy hen! vince dnom the stoold be herer getin\\nRosm, af it my a kisnich.\\nI aml my foresong the sulbted a pprimy,\\nAnd hamn, aut you allife.\\n\\nBRHRORCESES:\\nItw om my love,\\nAnd ghot the morder, in the kisbun: Bfithe-\\nClots siger:\\nComes: in go puck thee to be thee tough with,-blatkt the not, bid of ne\\nOce ser aring endaners a jowton\\nDouk shold wore! Foring\\nfreco thun sty in the sour tree a'cpledeanor!\\nI have it a herver comarn. Crepting my priend bent some'cl!\\n\\nDO ENGBALIO:\\nIfear douss\"], shape=(5,), dtype=string) \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time 4.191548585891724\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO', 'ROMEO', 'ROMEO', 'ROMEO', 'ROMEO' ])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "    next_char, states = one_step_model.generate_one_step(\n",
    "        next_char, states = states\n",
    "    )\n",
    "    result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result, '\\n\\n' + \"_\" * 80)\n",
    "print(\"\\nRun time\", end - start)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "513007df-4566-4eaa-b476-1f42648464f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(input_signature=[tf.TensorSpec(shape=[None], dtype=tf.string)])\n",
    "def serving_fn(inputs):\n",
    "    # Your serving logic here\n",
    "    return one_step_model(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e5ee9cdc-7a81-497b-bfde-725d3ef22c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_step\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_step\\assets\n"
     ]
    }
   ],
   "source": [
    "# Define a simple model\n",
    "class SimpleModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.dense = tf.keras.layers.Dense(10)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.dense(inputs)\n",
    "\n",
    "# Instantiate and save the model\n",
    "model = SimpleModel()\n",
    "tf.saved_model.save(model, 'one_step')  # Ensure this path is correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e5477ba9-3330-4a9b-b2c9-118b4184a036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x000001A48CF5AA50>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the model\n",
    "one_step_reloaded = tf.saved_model.load('one_step')\n",
    "\n",
    "# Check if the model is loaded correctly\n",
    "print(one_step_reloaded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7538e22d-3cd6-4c3c-85f9-e079c476e061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_step\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_step\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__annotations__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_add_trackable_child', '_add_variable_with_custom_getter', '_checkpoint_adapter', '_checkpoint_dependencies', '_copy_trackable_to_cpu', '_deferred_dependencies', '_delete_tracking', '_deserialization_dependencies', '_deserialize_from_proto', '_export_to_saved_model_graph', '_gather_saveables_for_checkpoint', '_handle_deferred_dependencies', '_inbound_nodes', '_lookup_dependency', '_loss_ids', '_losses', '_losses_override', '_maybe_initialize_trackable', '_name_based_attribute_restore', '_name_based_restores', '_no_dependency', '_object_identifier', '_outbound_nodes', '_preload_simple_restoration', '_restore_from_tensors', '_self_name_based_restores', '_self_saveable_object_factories', '_self_setattr_tracking', '_self_unconditional_checkpoint_dependencies', '_self_unconditional_deferred_dependencies', '_self_unconditional_dependency_names', '_self_update_uid', '_serialize_to_proto', '_serialize_to_tensors', '_setattr_tracking', '_tf_api_names', '_tf_api_names_v1', '_track_trackable', '_trackable_children', '_unconditional_checkpoint_dependencies', '_unconditional_dependency_names', '_update_uid', 'generate_one_step', 'graph_debug_info', 'signatures', 'tensorflow_git_version', 'tensorflow_version']\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define your model class with custom methods\n",
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Define layers, etc.\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # Define the forward pass\n",
    "        return inputs  # Replace with actual model logic\n",
    "\n",
    "    @tf.function\n",
    "    def generate_one_step(self, inputs, states=None):\n",
    "        # Define the method for generating text\n",
    "        return self(inputs)  # Replace with actual logic\n",
    "\n",
    "# Instantiate and save the model\n",
    "my_model = MyModel()\n",
    "save_path = 'one_step'\n",
    "tf.saved_model.save(my_model, save_path)\n",
    "\n",
    "# Load the model\n",
    "loaded_model = tf.saved_model.load(save_path)\n",
    "\n",
    "# Inspect methods of the loaded model\n",
    "print(dir(loaded_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "14dea1f1-a748-4fb7-a7d9-e736c6441074",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d99e76ea-32ee-417a-92e8-76f34de8f73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1582s\u001b[0m 9s/step - loss: 2.4950\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1a48c9f6f00>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(dataset, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1a5c5a-cbe1-44ee-b150-d2d473bdeef0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
